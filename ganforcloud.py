# -*- coding: utf-8 -*-
"""GANFORCLOUD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pkd8GC4c_nsz6-3KsRGpEXWnfxKsgRAQ
"""

!rm -rf ~/.kaggle
!mkdir ~/.kaggle
!touch ~/.kaggle/kaggle.json

from dotenv import load_dotenv
import os
load_dotenv()




api_token = {"username":os.getenv("USERNAME"),"key":os.getenv("KEY")}

import json

with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d cdeotte/cloud-images-resized
!unzip /content/cloud-images-resized.zip

!cp -a test_images_128x192/. train_images_128x192/
!rm -rf test_images_128x192 test_images_192x288 test_images_256x384 test_images_320x480 test_images_384x576 test_images_64x96  
!rm -rf train_images_320x480 train_images_192x288 train_images_256x384  train_images_384x576 train_images_64x96

!mkdir data
!mv train_images_128x192/ class/
!mv class/ data/


import torch
import torch.nn as nn
from torch import save , load
import torch.nn.functional as F
import matplotlib.pyplot as plt
import torchvision.transforms as tt
from PIL import Image
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader



DATA_DIR = "/content/data"
# print(os.listdir(DATA_DIR))
print("Amount of Image we are training on : ",len(os.listdir(DATA_DIR+'/class')))

# data preprocessing 
batch_size = 16
image_size = 128
norm_stats = (0.5) , (0.5)


train_ds = ImageFolder(DATA_DIR, transform=tt.Compose([ 
                                                        tt.Resize((image_size , image_size)),
                                                        tt.Grayscale(),
                                                        tt.ToTensor(),
                                                        tt.Normalize(*norm_stats)]))
train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)

def denorm(img_tensors):
    return img_tensors * norm_stats[1][0] + norm_stats[0][0]


device = "cuda" if torch.cuda.is_available() else "cpu"

class Generator(nn.Module):
  def __init__(self):
    super(Generator , self).__init__()
    self.model = nn.Sequential(
        # in: batch_size x 1 x 1 x 1
        nn.ConvTranspose2d(1, 512, kernel_size=4, stride=1, padding=0, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(True),
        # out: batch_size x 512 x 4 x 4

        nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(True),
        # out: batch_size x 256 x 8 x 8

        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(True),
        # out: batch_size x 128 x 16 x 16

        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(64),
        nn.ReLU(True),
        # out: batch_size x 64 x 32 x 32

        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(32),
        nn.ReLU(True),
        # out: batch_size x 32 x 64 x 64


        nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1, bias=False),
        nn.Tanh()
        # out: batch_size x 1 x 128 x 128
    )

  def forward(self,x):
    return self.model(x)

class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator , self).__init__()
    self.model = nn.Sequential(
        # in : 1 x 128 x 128

        nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(64),
        nn.LeakyReLU(0.2, inplace=True),
        # out: 64 x 64 x 64

        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.LeakyReLU(0.2, inplace=True),
        # out: 128 x 32 x 32

        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(256),
        nn.LeakyReLU(0.2, inplace=True),
        # out: 256 x 16 x 16

        nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(512),
        nn.LeakyReLU(0.2, inplace=True),
        # out: 512 x 8 x 8

        nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(512),
        nn.LeakyReLU(0.2, inplace=True),
        # out: 512 x 4 x 4

        nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),
        # out: 1 x 1 x 1

        nn.Flatten(),
        nn.Sigmoid()
    )

  def forward(self , x):
    return self.model(x)

generator = Generator().to(device)
discriminator = Discriminator().to(device)

def train_discriminator(real_images, opt_d):
    # Clear discriminator gradients
    opt_d.zero_grad()

    # Pass real images through discriminator
    real_preds = discriminator(real_images)
    real_targets = torch.ones(real_images.size(0), 1, device=device)
    real_loss = F.binary_cross_entropy(real_preds, real_targets)
    real_score = torch.mean(real_preds).item()
    
    # Generate fake images
    latent = torch.randn(batch_size, 1, 1, 1, device=device)
    fake_images = generator(latent)

    # Pass fake images through discriminator
    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)
    fake_preds = discriminator(fake_images)
    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets).to(device)
    fake_score = torch.mean(fake_preds).item()

    # Update discriminator weights
    loss = real_loss + fake_loss
    loss.backward()
    opt_d.step()
    return loss.item(), real_score, fake_score

def train_generator(opt_g):
    # Clear generator gradients
    opt_g.zero_grad()
    
    # Generate fake images
    latent = torch.randn(batch_size, 1, 1, 1, device=device)
    fake_images = generator(latent)
    
    # Try to fool the discriminator
    preds = discriminator(fake_images)
    targets = torch.ones(batch_size, 1, device=device)
    loss = F.binary_cross_entropy(preds, targets).to(device)
    
    # Update generator weights
    loss.backward()
    opt_g.step()
    
    return loss.item()

lr = 0.0002
epochs = 100

torch.cuda.empty_cache()
    
    # Losses & scores
losses_g = []
losses_d = []
real_scores = []
fake_scores = []


# Create optimizers
opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))
opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))



for epoch in range(epochs):
    for real_images, _ in train_dl:
        # Train discriminator
        real_images =  real_images.to(device)
        loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)
        # Train generator
        loss_g = train_generator(opt_g)
        
    # Record losses & scores
    losses_g.append(loss_g)
    losses_d.append(loss_d)
    real_scores.append(real_score)
    fake_scores.append(fake_score)
    
    # Log losses & scores (last batch)
    print("Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}".format(
        epoch+1, epochs, loss_g, loss_d, real_score, fake_score))

    # Save generated images
    # save_samples(epoch+start_idx, fixed_latent, show=False)

with open("generator_model.pt" , "wb") as f:
  save(generator.state_dict(),f)

with open("discriminator_model.pt" , "wb") as f:
  save(discriminator.state_dict(), f)

os.makedirs('saved_images', exist_ok=True)
# generating a single image
transform = tt.ToPILImage()


for i in range(600):
  latent = torch.randn(batch_size, 1, 1, 1, device=device)
  image = generator(latent)
  for j in range(16):
    img = transform(image[j])
    filename = f'image_{i}_{j}.jpg'
    filepath = os.path.join('saved_images', filename)
    img.save(filepath)

FOLDER = "/content/saved_images"
print("Amount of Image we are training on : ",len(os.listdir(FOLDER)))

!rm -rf saved_images/

import matplotlib.pyplot as plt
latent = torch.randn(batch_size, 1, 1, 1, device=device)
fake_image = generator(latent)
transform = tt.ToPILImage()
image = transform(fake_image[1])
plt.imshow(image)
plt.show()

